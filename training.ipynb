{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTzvZthgHlA2",
        "outputId": "2a7b605e-be39-4d8b-e36b-566331f3b98e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDevice: cuda Tesla T4\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu --quiet\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device, torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "r-JmX9-TIOlO",
        "outputId": "2d852276-edad-4258-b894-50a49a700a6a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-be44ffa9-068c-4700-b103-f9fabc87d163\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-be44ffa9-068c-4700-b103-f9fabc87d163\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test_bpe.json to test_bpe.json\n",
            "Saving train_bpe.json to train_bpe.json\n",
            "Saving val_bpe.json to val_bpe.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload your JSON files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PC7hRpxNJJ9b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2  # adjustable\n",
        "\n",
        "class BpeDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        src = torch.tensor(item['urdu_ids'], dtype=torch.long)\n",
        "        trg = torch.tensor(item['roman_ids'], dtype=torch.long)\n",
        "        return src, trg\n",
        "\n",
        "def collate_fn(batch):\n",
        "    srcs, trgs = zip(*batch)\n",
        "    src_lens = torch.tensor([len(s) for s in srcs])\n",
        "    trg_lens = torch.tensor([len(t) for t in trgs])\n",
        "    src_pad = pad_sequence(srcs, batch_first=True, padding_value=PAD_IDX)\n",
        "    trg_pad = pad_sequence(trgs, batch_first=True, padding_value=PAD_IDX)\n",
        "    return src_pad, src_lens, trg_pad, trg_lens\n",
        "\n",
        "# Paths for Colab\n",
        "train_ds = BpeDataset(\"/content/train_bpe.json\")\n",
        "val_ds   = BpeDataset(\"/content/val_bpe.json\")\n",
        "test_ds  = BpeDataset(\"/content/test_bpe.json\")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Loss function (ignores PAD tokens during training)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md6_BQtnMC-j",
        "outputId": "c71f0608-33c2-41fb-a79e-644ccf828b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC batch: torch.Size([16, 11])\n",
            "TRG batch: torch.Size([16, 16])\n",
            "SRC lens: tensor([10,  8,  7, 10, 11,  8, 10,  8, 11, 11])\n",
            "TRG lens: tensor([12, 10, 12, 13, 14, 16, 13, 14, 13, 16])\n"
          ]
        }
      ],
      "source": [
        "#for checking only\n",
        "src, src_lens, trg, trg_lens = next(iter(train_loader))\n",
        "print(\"SRC batch:\", src.shape)\n",
        "print(\"TRG batch:\", trg.shape)\n",
        "print(\"SRC lens:\", src_lens[:10])\n",
        "print(\"TRG lens:\", trg_lens[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GWGeYuLMMdm",
        "outputId": "3fae9b4f-7200-41b2-a83b-cc9e45afaef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SRC vocab size: 15999\n",
            "TRG vocab size: 15978\n",
            "SRC shape: torch.Size([16, 11])\n",
            "TRG shape: torch.Size([16, 14])\n",
            "Sample loss: 10.13171100616455\n"
          ]
        }
      ],
      "source": [
        "# Find actual vocab sizes from dataset\n",
        "max_src_id = max(max(item['urdu_ids']) for item in train_ds.data)\n",
        "max_trg_id = max(max(item['roman_ids']) for item in train_ds.data)\n",
        "\n",
        "SRC_VOCAB_SIZE = max_src_id + 1\n",
        "TRG_VOCAB_SIZE = max_trg_id + 1\n",
        "\n",
        "print(\"SRC vocab size:\", SRC_VOCAB_SIZE)\n",
        "print(\"TRG vocab size:\", TRG_VOCAB_SIZE)\n",
        "\n",
        "# Sanity check batch\n",
        "src_pad, src_lens, trg_pad, trg_lens = next(iter(train_loader))\n",
        "print(\"SRC shape:\", src_pad.shape)\n",
        "print(\"TRG shape:\", trg_pad.shape)\n",
        "\n",
        "# Create fake logits with correct vocab size\n",
        "logits = torch.randn(trg_pad.size(0), trg_pad.size(1), TRG_VOCAB_SIZE)\n",
        "\n",
        "# Compute loss ignoring PAD_IDX\n",
        "out_dim = logits.shape[-1]\n",
        "loss = criterion(logits.reshape(-1, out_dim), trg_pad.reshape(-1))\n",
        "print(\"Sample loss:\", loss.item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        # encoder is bidirectional → enc_hid_dim*2\n",
        "        self.enc_proj = nn.Linear(enc_hid_dim*2, dec_hid_dim)\n",
        "        self.attn = nn.Linear(dec_hid_dim, dec_hid_dim)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask=None):\n",
        "        # encoder_outputs: [batch, src_len, enc_hid*2]\n",
        "        # hidden: [batch, dec_hid]\n",
        "\n",
        "        # project encoder outputs → [batch, src_len, dec_hid]\n",
        "        proj_enc = self.enc_proj(encoder_outputs)\n",
        "\n",
        "        # transform hidden → [batch, dec_hid, 1]\n",
        "        query = self.attn(hidden).unsqueeze(2)\n",
        "\n",
        "        # scores: [batch, src_len]\n",
        "        scores = torch.bmm(proj_enc, query).squeeze(2)\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_weights = torch.softmax(scores, dim=1)\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), proj_enc).squeeze(1)  # [batch, dec_hid]\n",
        "\n",
        "        return context, attn_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "d2ndb2c7vNoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSgqdZYzPAqb",
        "outputId": "50f8fea4-075c-4c2d-9306-5784330694f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "SRC vocab size: 15999\n",
            "TRG vocab size: 15978\n"
          ]
        }
      ],
      "source": [
        "# ---------- paste this whole block into one Colab cell ----------\n",
        "import json, math, torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from torch.optim import Adam\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "import sacrebleu\n",
        "\n",
        "# ---- constants ----\n",
        "PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---- Dataset loader (expects /content/train_bpe.json etc) ----\n",
        "class BpeDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            self.data = json.load(f)\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        it = self.data[idx]\n",
        "        src = torch.tensor(it['urdu_ids'], dtype=torch.long)\n",
        "        trg = torch.tensor(it['roman_ids'], dtype=torch.long)\n",
        "        return src, trg\n",
        "\n",
        "def collate_fn(batch):\n",
        "    srcs, trgs = zip(*batch)\n",
        "    src_lens = torch.tensor([len(s) for s in srcs], dtype=torch.long)\n",
        "    trg_lens = torch.tensor([len(t) for t in trgs], dtype=torch.long)\n",
        "    src_pad = pad_sequence(srcs, batch_first=True, padding_value=PAD_IDX)\n",
        "    trg_pad = pad_sequence(trgs, batch_first=True, padding_value=PAD_IDX)\n",
        "    return src_pad, src_lens, trg_pad, trg_lens\n",
        "\n",
        "train_ds = BpeDataset(\"/content/train_bpe.json\")\n",
        "val_ds   = BpeDataset(\"/content/val_bpe.json\")\n",
        "test_ds  = BpeDataset(\"/content/test_bpe.json\")\n",
        "\n",
        "BATCH = 64\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# ---- utilities: build vocab sizes and id->token maps from your JSON files ----\n",
        "def get_vocab_size(path, id_key):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    max_id = max(max(item[id_key]) for item in data)\n",
        "    return max_id + 1\n",
        "\n",
        "def build_id2tok(paths, id_key, token_key):\n",
        "    id2tok = {}\n",
        "    for path in paths:\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "        for item in data:\n",
        "            ids = item[id_key]\n",
        "            toks = item[token_key]\n",
        "            for i, idx in enumerate(ids):\n",
        "                k = str(idx)\n",
        "                # if mapping conflict occurs, keep first seen mapping (should be consistent)\n",
        "                if k not in id2tok:\n",
        "                    id2tok[k] = toks[i] if i < len(toks) else \"<UNK>\"\n",
        "    # ensure special tokens\n",
        "    id2tok.setdefault(str(PAD_IDX), \"<PAD>\")\n",
        "    id2tok.setdefault(str(SOS_IDX), \"<SOS>\")\n",
        "    id2tok.setdefault(str(EOS_IDX), \"<EOS>\")\n",
        "    return id2tok\n",
        "\n",
        "INPUT_DIM  = get_vocab_size(\"/content/train_bpe.json\", \"urdu_ids\")\n",
        "OUTPUT_DIM = get_vocab_size(\"/content/train_bpe.json\", \"roman_ids\")\n",
        "tgt_id2tok = build_id2tok([\"/content/train_bpe.json\",\"/content/val_bpe.json\",\"/content/test_bpe.json\"], \"roman_ids\", \"roman_tokens\")\n",
        "\n",
        "print(\"SRC vocab size:\", INPUT_DIM)\n",
        "print(\"TRG vocab size:\", OUTPUT_DIM)\n",
        "\n",
        "# ---- Model definitions (BiLSTM encoder 2 layers, LSTM decoder 4 layers) ----\n",
        "class EncoderBiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers,\n",
        "                            bidirectional=True, batch_first=True, dropout=dropout)\n",
        "        self.fc_h = nn.Linear(hid_dim*2, hid_dim)\n",
        "        self.fc_c = nn.Linear(hid_dim*2, hid_dim)\n",
        "    def forward(self, src, src_len):\n",
        "        emb = self.embedding(src)                            # [b, src_len, emb]\n",
        "        packed = pack_padded_sequence(emb, src_len.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_out, (h, c) = self.lstm(packed)\n",
        "        enc_out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
        "        h_cat = torch.cat((h[-2,:,:], h[-1,:,:]), dim=1)     # [b, hid*2]\n",
        "        c_cat = torch.cat((c[-2,:,:], c[-1,:,:]), dim=1)\n",
        "        h0 = torch.tanh(self.fc_h(h_cat)).unsqueeze(0)       # [1,b,hid]\n",
        "        c0 = torch.tanh(self.fc_c(c_cat)).unsqueeze(0)\n",
        "        return enc_out, (h0, c0)\n",
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=2, dropout=0.3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim + hid_dim, hid_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.n_layers = n_layers\n",
        "        self.attention = Attention(hid_dim, hid_dim)\n",
        "\n",
        "    def forward_step(self, input_token, hidden, cell, encoder_outputs, mask=None):\n",
        "        emb = self.embedding(input_token).unsqueeze(1)  # [b,1,emb]\n",
        "        context, attn_weights = self.attention(hidden[-1], encoder_outputs, mask)  # use last layer hidden\n",
        "        context = context.unsqueeze(1)  # [b,1,hid]\n",
        "        rnn_input = torch.cat([emb, context], dim=2)   # [b,1,emb+hid]\n",
        "        out, (h, c) = self.lstm(rnn_input, (hidden, cell))\n",
        "        pred = self.fc_out(out.squeeze(1))\n",
        "        return pred, h, c, attn_weights\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, src_len, trg=None, teacher_forcing=0.5, max_len=100):\n",
        "        bsz = src.size(0)\n",
        "        trg_len = trg.size(1) if trg is not None else max_len\n",
        "        vocab_size = self.decoder.fc_out.out_features\n",
        "        outputs = torch.zeros(bsz, trg_len, vocab_size, device=self.device)\n",
        "\n",
        "        enc_out, (h, c) = self.encoder(src, src_len)\n",
        "\n",
        "        input_token = trg[:,0] if trg is not None else torch.full((bsz,), SOS_IDX, dtype=torch.long, device=self.device)\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            pred, h, c, _ = self.decoder.forward_step(input_token, h, c, enc_out)\n",
        "            outputs[:, t, :] = pred\n",
        "            if trg is not None:\n",
        "                if torch.rand(1).item() < teacher_forcing:\n",
        "                    input_token = trg[:, t]\n",
        "                else:\n",
        "                    input_token = pred.argmax(1)\n",
        "            else:\n",
        "                input_token = pred.argmax(1)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJFGWzzfSgyH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---- training helpers ----\n",
        "def ids_to_string(ids, id2tok):\n",
        "    toks = []\n",
        "    for idx in ids:\n",
        "        if idx == PAD_IDX: continue\n",
        "        if idx == SOS_IDX: continue\n",
        "        if idx == EOS_IDX: break\n",
        "        toks.append(id2tok.get(str(idx), \"<UNK>\"))\n",
        "    s = \"\".join(toks).replace(\"▁\", \" \").strip()\n",
        "    return s\n",
        "\n",
        "def levenshtein(a,b):\n",
        "    n,m = len(a), len(b)\n",
        "    if n==0: return m\n",
        "    dp = list(range(m+1))\n",
        "    for i in range(1,n+1):\n",
        "        prev, dp[0] = dp[0], i\n",
        "        for j in range(1,m+1):\n",
        "            cur = min(dp[j] + 1, prev + (a[i-1] != b[j-1]), dp[j-1] + 1)\n",
        "            prev, dp[j] = dp[j], cur\n",
        "    return dp[m]\n",
        "\n",
        "def compute_metrics(preds, refs, val_loss):\n",
        "    bleu = sacrebleu.corpus_bleu(preds, [refs]).score if len(preds)>0 else 0.0\n",
        "    cers = [levenshtein(p,r)/max(1,len(r)) for p,r in zip(preds, refs)] if len(preds)>0 else [1.0]\n",
        "    cer = sum(cers)/len(cers)\n",
        "    try:\n",
        "        ppl = math.exp(val_loss)\n",
        "    except OverflowError:\n",
        "        ppl = float('inf')\n",
        "    return bleu, cer, ppl\n",
        "\n",
        "# ---- train / eval functions (AMP-safe) ----\n",
        "def train_epoch(model, loader, optimizer, criterion, scaler, tf_ratio):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for src, src_len, trg, _ in tqdm(loader, desc=\"train\", leave=False):\n",
        "        src, src_len, trg = src.to(device), src_len.to(device), trg.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            outputs = model(src, src_len, trg, teacher_forcing=tf_ratio)\n",
        "            out_dim = outputs.shape[-1]\n",
        "            loss = criterion(outputs[:,1:,:].reshape(-1, out_dim), trg[:,1:].reshape(-1))\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader, criterion, id2tok):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    preds_str = []\n",
        "    refs_str = []\n",
        "    with torch.no_grad():\n",
        "        for src, src_len, trg, _ in tqdm(loader, desc=\"eval\", leave=False):\n",
        "            src, src_len, trg = src.to(device), src_len.to(device), trg.to(device)\n",
        "            outputs = model(src, src_len, trg=None, teacher_forcing=0.0, max_len=trg.size(1))\n",
        "            out_dim = outputs.shape[-1]\n",
        "            loss = criterion(outputs[:,:trg.size(1),:].reshape(-1,out_dim), trg.reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "            top = outputs.argmax(-1).cpu().tolist()\n",
        "            for i in range(len(top)):\n",
        "                preds_str.append(ids_to_string(top[i], id2tok))\n",
        "                refs_str.append(ids_to_string(trg[i].cpu().tolist(), id2tok))\n",
        "    return total_loss / len(loader), preds_str, refs_str\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsGrjRPsSype"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def run_training(INPUT_DIM, OUTPUT_DIM,\n",
        "                 emb_dim=256, hid_dim=512,\n",
        "                 enc_layers=4, dec_layers=4,\n",
        "                 dropout=0.3, lr=5e-4,\n",
        "                 epochs=12, save_dir=\"/content\",\n",
        "                 resume_from=None):\n",
        "\n",
        "    # 1) Build model with attention\n",
        "    encoder = EncoderBiLSTM(INPUT_DIM, emb_dim, hid_dim,\n",
        "                            n_layers=enc_layers, dropout=dropout, pad_idx=PAD_IDX)\n",
        "    decoder = DecoderWithAttention(OUTPUT_DIM, emb_dim, hid_dim,\n",
        "                                   n_layers=dec_layers, dropout=dropout, pad_idx=PAD_IDX)\n",
        "    model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-6)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n",
        "\n",
        "    scaler = GradScaler()   # AMP scaler\n",
        "\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_epoch = -1\n",
        "\n",
        "    # Resume from checkpoint if provided\n",
        "    if resume_from is not None:\n",
        "        checkpoint = torch.load(resume_from, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "        scaler.load_state_dict(checkpoint[\"scaler_state\"])\n",
        "        start_epoch = checkpoint[\"epoch\"] + 1\n",
        "        best_val_loss = checkpoint.get(\"best_val_loss\", float(\"inf\"))\n",
        "        best_epoch = checkpoint.get(\"best_epoch\", -1)\n",
        "        print(f\"Resumed training from epoch {start_epoch} using {resume_from}\")\n",
        "\n",
        "    # 2) Track history + scheduler\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"bleu\": [], \"cer\": [], \"ppl\": []}\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", factor=0.5, patience=3\n",
        "    )\n",
        "\n",
        "    # 3) Training loop\n",
        "    for epoch in range(start_epoch, start_epoch + epochs):\n",
        "        tf = max(0.0, 1 - epoch / max(1, start_epoch + epochs))   # teacher forcing decay\n",
        "\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, scaler, tf)\n",
        "        val_loss, preds, refs = evaluate(model, val_loader, criterion, tgt_id2tok)\n",
        "        bleu, cer, ppl = compute_metrics(preds, refs, val_loss)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"bleu\"].append(bleu)\n",
        "        history[\"cer\"].append(cer)\n",
        "        history[\"ppl\"].append(ppl)\n",
        "\n",
        "        print(f\"[E{epoch+1}] train_loss={train_loss:.4f} \"\n",
        "              f\"val_loss={val_loss:.4f} BLEU={bleu:.2f} CER={cer:.4f} PPL={ppl:.2f}\")\n",
        "\n",
        "        # save full checkpoint\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"scaler_state\": scaler.state_dict(),\n",
        "            \"best_val_loss\": best_val_loss,\n",
        "            \"best_epoch\": best_epoch\n",
        "        }\n",
        "        torch.save(checkpoint, f\"{save_dir}/checkpoint_epoch{epoch+1}.pt\")\n",
        "\n",
        "        # save best checkpoint\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch + 1\n",
        "            torch.save(checkpoint, f\"{save_dir}/best_checkpoint.pt\")\n",
        "            print(f\"==> Saved best model at epoch {best_epoch} (val_loss={best_val_loss:.4f})\")\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    # 4) (Optional) Plotting can go here if you want curves\n",
        "\n",
        "    return history, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrnTTAM-Fm6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54a4b7e8-d55c-4199-dcf5-47994b9f9b5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2667450489.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()   # correct usage\n",
            "train:   0%|          | 0/165 [00:00<?, ?it/s]/tmp/ipython-input-2833786018.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E1] train_loss=6.6281 val_loss=6.6963 BLEU=0.03 CER=0.7517 PPL=809.44\n",
            "==> Saved best model at epoch 1 (val_loss=6.6963)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E2] train_loss=6.3598 val_loss=6.6922 BLEU=0.04 CER=0.7503 PPL=806.12\n",
            "==> Saved best model at epoch 2 (val_loss=6.6922)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E3] train_loss=6.3292 val_loss=6.7029 BLEU=0.03 CER=0.7940 PPL=814.80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E4] train_loss=6.2776 val_loss=6.7267 BLEU=0.03 CER=0.8082 PPL=834.40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5] train_loss=6.2363 val_loss=6.6827 BLEU=0.04 CER=0.7488 PPL=798.49\n",
            "==> Saved best model at epoch 5 (val_loss=6.6827)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E6] train_loss=6.1999 val_loss=6.7344 BLEU=0.03 CER=0.7796 PPL=840.81\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E7] train_loss=6.1894 val_loss=6.6761 BLEU=0.03 CER=0.7517 PPL=793.22\n",
            "==> Saved best model at epoch 7 (val_loss=6.6761)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E8] train_loss=6.1917 val_loss=6.6782 BLEU=0.03 CER=0.7544 PPL=794.92\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E9] train_loss=6.1799 val_loss=6.6417 BLEU=0.04 CER=0.7487 PPL=766.39\n",
            "==> Saved best model at epoch 9 (val_loss=6.6417)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E10] train_loss=6.1575 val_loss=6.6445 BLEU=0.04 CER=0.8955 PPL=768.52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E11] train_loss=6.1652 val_loss=6.6144 BLEU=0.04 CER=0.7880 PPL=745.77\n",
            "==> Saved best model at epoch 11 (val_loss=6.6144)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E12] train_loss=6.1537 val_loss=6.5733 BLEU=0.05 CER=0.7531 PPL=715.74\n",
            "==> Saved best model at epoch 12 (val_loss=6.5733)\n"
          ]
        }
      ],
      "source": [
        "history, model = run_training(INPUT_DIM, OUTPUT_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_path, batch_size=64):\n",
        "    # load dataset\n",
        "    test_ds = BpeDataset(test_path)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # evaluate\n",
        "    test_loss, preds, refs = evaluate(model, test_loader, criterion, tgt_id2tok)\n",
        "    bleu, cer, ppl = compute_metrics(preds, refs, test_loss)\n",
        "\n",
        "    print(\"\\n=== Test Set Results ===\")\n",
        "    print(f\"Loss: {test_loss:.4f}\")\n",
        "    print(f\"BLEU: {bleu:.2f}\")\n",
        "    print(f\"CER:  {cer:.4f}\")\n",
        "    print(f\"PPL:  {ppl:.2f}\")\n",
        "\n",
        "    # Show some examples\n",
        "    for i in range(5):\n",
        "        print(f\"REF: {refs[i]}\")\n",
        "        print(f\"PRD: {preds[i]}\")\n",
        "        print(\"-\"*40)\n",
        "\n",
        "    return bleu, cer, ppl, preds, refs\n"
      ],
      "metadata": {
        "id": "erWvzrsRZGw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(model, urdu_ids, tgt_id2tok, max_len=100, ref_ids=None):\n",
        "    model.eval()\n",
        "    src = torch.tensor(urdu_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    src_len = torch.tensor([len(urdu_ids)], dtype=torch.long).to(device)\n",
        "\n",
        "    # forward (no teacher forcing)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(src, src_len, trg=None, teacher_forcing=0.0, max_len=max_len)\n",
        "        top = outputs.argmax(-1).squeeze(0).tolist()\n",
        "\n",
        "    # decode ids -> string\n",
        "    pred_str = ids_to_string(top, tgt_id2tok)\n",
        "\n",
        "    # optional metrics if reference provided\n",
        "    if ref_ids is not None:\n",
        "        ref_str = ids_to_string(ref_ids, tgt_id2tok)\n",
        "        bleu, cer, ppl = compute_metrics([pred_str], [ref_str], val_loss=0.0)\n",
        "        print(\"\\n=== Single Sentence Test ===\")\n",
        "        print(\"REF:\", ref_str)\n",
        "        print(\"PRD:\", pred_str)\n",
        "        print(f\"BLEU: {bleu:.2f}, CER: {cer:.4f}, PPL: {ppl:.2f}\")\n",
        "    else:\n",
        "        print(\"\\n=== Single Sentence Test ===\")\n",
        "        print(\"PRD:\", pred_str)\n",
        "\n",
        "    return pred_str\n"
      ],
      "metadata": {
        "id": "TkbQaEf-b4fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- load best model ---\n",
        "checkpoint = torch.load(\"/content/best_checkpoint.pt\", map_location=device)\n",
        "model.load_state_dict(checkpoint[\"model_state\"])\n",
        "model.to(device)\n",
        "\n",
        "# --- test on whole test_bpe.json ---\n",
        "test_model(model, \"/content/test_bpe.json\")\n",
        "\n",
        "# --- test on a single Urdu sentence ---\n",
        "# Example: take one example from test_ds\n",
        "sample = test_ds[0]\n",
        "urdu_ids = sample[0].tolist()     # input ids\n",
        "ref_ids  = sample[1].tolist()     # ground truth roman ids\n",
        "translate_sentence(model, urdu_ids, tgt_id2tok, ref_ids=ref_ids)\n"
      ],
      "metadata": {
        "id": "z8OxP1rub7MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history, model = run_training(INPUT_DIM, OUTPUT_DIM, emb_dim=512, hid_dim=512,\n",
        "                 enc_layers=4, dec_layers=4,\n",
        "                 dropout=0.3, lr=1e-4, )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NlBjn8QcZJK",
        "outputId": "29500ec2-db52-4383-b087-276392159cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2667450489.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()   # correct usage\n",
            "train:   0%|          | 0/165 [00:00<?, ?it/s]/tmp/ipython-input-2833786018.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E1] train_loss=7.0891 val_loss=6.7016 BLEU=0.03 CER=0.7700 PPL=813.70\n",
            "==> Saved best model at epoch 1 (val_loss=6.7016)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E2] train_loss=6.3436 val_loss=6.7194 BLEU=0.04 CER=0.7712 PPL=828.31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E3] train_loss=6.3019 val_loss=6.6952 BLEU=0.04 CER=0.7546 PPL=808.48\n",
            "==> Saved best model at epoch 3 (val_loss=6.6952)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E4] train_loss=6.2775 val_loss=6.6530 BLEU=0.04 CER=0.7508 PPL=775.08\n",
            "==> Saved best model at epoch 4 (val_loss=6.6530)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5] train_loss=6.2576 val_loss=6.7048 BLEU=0.04 CER=0.7604 PPL=816.35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E6] train_loss=6.2409 val_loss=6.6127 BLEU=0.04 CER=0.7386 PPL=744.48\n",
            "==> Saved best model at epoch 6 (val_loss=6.6127)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E7] train_loss=6.2302 val_loss=6.6249 BLEU=0.04 CER=0.7424 PPL=753.66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E8] train_loss=6.2121 val_loss=6.6023 BLEU=0.04 CER=0.7462 PPL=736.75\n",
            "==> Saved best model at epoch 8 (val_loss=6.6023)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E9] train_loss=6.1968 val_loss=6.5817 BLEU=0.04 CER=0.7576 PPL=721.77\n",
            "==> Saved best model at epoch 9 (val_loss=6.5817)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E10] train_loss=6.1655 val_loss=6.5646 BLEU=0.04 CER=0.8667 PPL=709.53\n",
            "==> Saved best model at epoch 10 (val_loss=6.5646)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E11] train_loss=6.1319 val_loss=6.5466 BLEU=0.04 CER=0.8644 PPL=696.89\n",
            "==> Saved best model at epoch 11 (val_loss=6.5466)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E12] train_loss=6.0952 val_loss=6.5386 BLEU=0.05 CER=0.8360 PPL=691.32\n",
            "==> Saved best model at epoch 12 (val_loss=6.5386)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history, model = run_training(INPUT_DIM, OUTPUT_DIM, emb_dim=512, hid_dim=512,\n",
        "                 enc_layers=4, dec_layers=4,\n",
        "                 dropout=0.3, lr=1e-4, resume_from=\"/content/best_checkpoint.pt\", epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa7RQInicve0",
        "outputId": "f6401036-5813-44b8-d544-9fa3d1e452d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2667450489.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()   # correct usage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumed training from epoch 12 using /content/best_checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train:   0%|          | 0/165 [00:00<?, ?it/s]/tmp/ipython-input-2833786018.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E13] train_loss=6.0442 val_loss=6.5260 BLEU=0.06 CER=0.9293 PPL=682.66\n",
            "==> Saved best model at epoch 13 (val_loss=6.5260)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E14] train_loss=6.0021 val_loss=6.4947 BLEU=0.07 CER=0.8145 PPL=661.63\n",
            "==> Saved best model at epoch 14 (val_loss=6.4947)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E15] train_loss=5.9628 val_loss=6.5067 BLEU=0.10 CER=0.8138 PPL=669.58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E16] train_loss=5.9139 val_loss=6.5039 BLEU=0.10 CER=0.8632 PPL=667.77\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E17] train_loss=5.8666 val_loss=6.4739 BLEU=0.11 CER=0.8886 PPL=648.03\n",
            "==> Saved best model at epoch 17 (val_loss=6.4739)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E18] train_loss=5.8193 val_loss=6.4185 BLEU=0.13 CER=0.8216 PPL=613.07\n",
            "==> Saved best model at epoch 18 (val_loss=6.4185)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E19] train_loss=5.7766 val_loss=6.4172 BLEU=0.13 CER=0.8912 PPL=612.28\n",
            "==> Saved best model at epoch 19 (val_loss=6.4172)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E20] train_loss=5.7343 val_loss=6.3859 BLEU=0.12 CER=0.8020 PPL=593.40\n",
            "==> Saved best model at epoch 20 (val_loss=6.3859)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E21] train_loss=5.6908 val_loss=6.3558 BLEU=0.14 CER=0.7928 PPL=575.81\n",
            "==> Saved best model at epoch 21 (val_loss=6.3558)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E22] train_loss=5.6411 val_loss=6.3215 BLEU=0.17 CER=0.7282 PPL=556.38\n",
            "==> Saved best model at epoch 22 (val_loss=6.3215)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E23] train_loss=5.5897 val_loss=6.3180 BLEU=0.16 CER=0.7430 PPL=554.45\n",
            "==> Saved best model at epoch 23 (val_loss=6.3180)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E24] train_loss=5.5421 val_loss=6.2746 BLEU=0.27 CER=0.7314 PPL=530.93\n",
            "==> Saved best model at epoch 24 (val_loss=6.2746)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E25] train_loss=5.4948 val_loss=6.2464 BLEU=0.36 CER=0.7026 PPL=516.14\n",
            "==> Saved best model at epoch 25 (val_loss=6.2464)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E26] train_loss=5.4445 val_loss=6.2614 BLEU=0.39 CER=0.7290 PPL=523.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E27] train_loss=5.3981 val_loss=6.2324 BLEU=0.53 CER=0.7028 PPL=508.96\n",
            "==> Saved best model at epoch 27 (val_loss=6.2324)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E28] train_loss=5.3562 val_loss=6.1974 BLEU=0.73 CER=0.7005 PPL=491.45\n",
            "==> Saved best model at epoch 28 (val_loss=6.1974)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E29] train_loss=5.3072 val_loss=6.2023 BLEU=0.92 CER=0.7005 PPL=493.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E30] train_loss=5.2621 val_loss=6.1624 BLEU=0.92 CER=0.6885 PPL=474.57\n",
            "==> Saved best model at epoch 30 (val_loss=6.1624)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E31] train_loss=5.2176 val_loss=6.1901 BLEU=1.42 CER=0.6996 PPL=487.87\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E32] train_loss=5.1723 val_loss=6.1154 BLEU=1.55 CER=0.6741 PPL=452.80\n",
            "==> Saved best model at epoch 32 (val_loss=6.1154)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history, model = run_training(INPUT_DIM, OUTPUT_DIM, emb_dim=512, hid_dim=256,\n",
        "                 enc_layers=6, dec_layers=4,\n",
        "                 dropout=0.3, lr=1e-4, epochs=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VAMKHMSe3ki",
        "outputId": "c04892f0-3e45-4970-d69e-1febdb112a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2667450489.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()   # correct usage\n",
            "train:   0%|          | 0/165 [00:00<?, ?it/s]/tmp/ipython-input-2833786018.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E1] train_loss=7.5925 val_loss=6.7545 BLEU=0.00 CER=0.9081 PPL=857.94\n",
            "==> Saved best model at epoch 1 (val_loss=6.7545)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E2] train_loss=6.4043 val_loss=6.7107 BLEU=0.03 CER=0.7859 PPL=821.18\n",
            "==> Saved best model at epoch 2 (val_loss=6.7107)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E3] train_loss=6.3631 val_loss=6.6800 BLEU=0.04 CER=0.7485 PPL=796.29\n",
            "==> Saved best model at epoch 3 (val_loss=6.6800)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E4] train_loss=6.3319 val_loss=6.6673 BLEU=0.04 CER=0.7477 PPL=786.24\n",
            "==> Saved best model at epoch 4 (val_loss=6.6673)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5] train_loss=6.3076 val_loss=6.6398 BLEU=0.03 CER=0.7422 PPL=764.93\n",
            "==> Saved best model at epoch 5 (val_loss=6.6398)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E6] train_loss=6.2859 val_loss=6.6231 BLEU=0.03 CER=0.7395 PPL=752.26\n",
            "==> Saved best model at epoch 6 (val_loss=6.6231)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E7] train_loss=6.2661 val_loss=6.6141 BLEU=0.03 CER=0.7414 PPL=745.54\n",
            "==> Saved best model at epoch 7 (val_loss=6.6141)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E8] train_loss=6.2495 val_loss=6.6113 BLEU=0.04 CER=0.7501 PPL=743.44\n",
            "==> Saved best model at epoch 8 (val_loss=6.6113)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history, model = run_training(INPUT_DIM, OUTPUT_DIM, emb_dim=256, hid_dim=256,\n",
        "                 enc_layers=2, dec_layers=4,\n",
        "                 dropout=0.1, lr=1e-4, epochs=12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYWj56SBkmps",
        "outputId": "8dd864ca-164a-4f5a-bfc7-7745040303ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2667450489.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()   # correct usage\n",
            "train:   0%|          | 0/165 [00:00<?, ?it/s]/tmp/ipython-input-2833786018.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E1] train_loss=7.5720 val_loss=6.7509 BLEU=0.00 CER=0.9081 PPL=854.87\n",
            "==> Saved best model at epoch 1 (val_loss=6.7509)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E2] train_loss=6.3882 val_loss=6.7160 BLEU=0.03 CER=0.7469 PPL=825.49\n",
            "==> Saved best model at epoch 2 (val_loss=6.7160)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E3] train_loss=6.3329 val_loss=6.6835 BLEU=0.04 CER=0.7285 PPL=799.11\n",
            "==> Saved best model at epoch 3 (val_loss=6.6835)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E4] train_loss=6.2944 val_loss=6.6217 BLEU=0.04 CER=0.7297 PPL=751.26\n",
            "==> Saved best model at epoch 4 (val_loss=6.6217)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5] train_loss=6.2507 val_loss=6.6263 BLEU=0.05 CER=0.7591 PPL=754.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E6] train_loss=6.2119 val_loss=6.5613 BLEU=0.07 CER=0.7452 PPL=707.19\n",
            "==> Saved best model at epoch 6 (val_loss=6.5613)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E7] train_loss=6.1706 val_loss=6.5263 BLEU=0.07 CER=0.7315 PPL=682.87\n",
            "==> Saved best model at epoch 7 (val_loss=6.5263)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E8] train_loss=6.1241 val_loss=6.4928 BLEU=0.09 CER=0.7511 PPL=660.35\n",
            "==> Saved best model at epoch 8 (val_loss=6.4928)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E9] train_loss=6.0685 val_loss=6.4558 BLEU=0.11 CER=0.7367 PPL=636.38\n",
            "==> Saved best model at epoch 9 (val_loss=6.4558)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E10] train_loss=6.0180 val_loss=6.4446 BLEU=0.12 CER=0.7551 PPL=629.30\n",
            "==> Saved best model at epoch 10 (val_loss=6.4446)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E11] train_loss=5.9668 val_loss=6.3859 BLEU=0.16 CER=0.7246 PPL=593.40\n",
            "==> Saved best model at epoch 11 (val_loss=6.3859)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E12] train_loss=5.9187 val_loss=6.3680 BLEU=0.17 CER=0.7422 PPL=582.92\n",
            "==> Saved best model at epoch 12 (val_loss=6.3680)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history, model = run_training(INPUT_DIM, OUTPUT_DIM, emb_dim=256, hid_dim=256,\n",
        "                 enc_layers=2, dec_layers=4,\n",
        "                 dropout=0.1, lr=1e-4, resume_from=\"/content/best_checkpoint.pt\", epochs=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNGTqz_UmHU6",
        "outputId": "10ae70e9-b86e-4580-8d2f-781fe4e35514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2667450489.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()   # correct usage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumed training from epoch 12 using /content/best_checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtrain:   0%|          | 0/165 [00:00<?, ?it/s]/tmp/ipython-input-2833786018.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E13] train_loss=5.8661 val_loss=6.3230 BLEU=0.16 CER=0.7172 PPL=557.26\n",
            "==> Saved best model at epoch 13 (val_loss=6.3230)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E14] train_loss=5.8186 val_loss=6.3081 BLEU=0.20 CER=0.7300 PPL=549.03\n",
            "==> Saved best model at epoch 14 (val_loss=6.3081)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E15] train_loss=5.7716 val_loss=6.2629 BLEU=0.21 CER=0.7107 PPL=524.76\n",
            "==> Saved best model at epoch 15 (val_loss=6.2629)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E16] train_loss=5.7199 val_loss=6.2375 BLEU=0.23 CER=0.6992 PPL=511.58\n",
            "==> Saved best model at epoch 16 (val_loss=6.2375)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E17] train_loss=5.6712 val_loss=6.2187 BLEU=0.24 CER=0.7012 PPL=502.04\n",
            "==> Saved best model at epoch 17 (val_loss=6.2187)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E18] train_loss=5.6257 val_loss=6.2102 BLEU=0.41 CER=0.7256 PPL=497.79\n",
            "==> Saved best model at epoch 18 (val_loss=6.2102)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E19] train_loss=5.5796 val_loss=6.1698 BLEU=0.45 CER=0.7054 PPL=478.08\n",
            "==> Saved best model at epoch 19 (val_loss=6.1698)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E20] train_loss=5.5382 val_loss=6.1776 BLEU=0.47 CER=0.7064 PPL=481.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E21] train_loss=5.4969 val_loss=6.1251 BLEU=0.60 CER=0.6773 PPL=457.19\n",
            "==> Saved best model at epoch 21 (val_loss=6.1251)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E22] train_loss=5.4561 val_loss=6.1141 BLEU=0.65 CER=0.6855 PPL=452.20\n",
            "==> Saved best model at epoch 22 (val_loss=6.1141)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E23] train_loss=5.4179 val_loss=6.0904 BLEU=0.89 CER=0.6884 PPL=441.61\n",
            "==> Saved best model at epoch 23 (val_loss=6.0904)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E24] train_loss=5.3725 val_loss=6.0728 BLEU=1.16 CER=0.6956 PPL=433.92\n",
            "==> Saved best model at epoch 24 (val_loss=6.0728)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E25] train_loss=5.3337 val_loss=6.0675 BLEU=1.26 CER=0.6864 PPL=431.61\n",
            "==> Saved best model at epoch 25 (val_loss=6.0675)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E26] train_loss=5.2926 val_loss=6.0346 BLEU=1.50 CER=0.6708 PPL=417.64\n",
            "==> Saved best model at epoch 26 (val_loss=6.0346)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E27] train_loss=5.2525 val_loss=6.0312 BLEU=1.69 CER=0.6855 PPL=416.23\n",
            "==> Saved best model at epoch 27 (val_loss=6.0312)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E28] train_loss=5.2123 val_loss=5.9987 BLEU=2.09 CER=0.6720 PPL=402.90\n",
            "==> Saved best model at epoch 28 (val_loss=5.9987)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E29] train_loss=5.1768 val_loss=5.9857 BLEU=2.42 CER=0.6649 PPL=397.69\n",
            "==> Saved best model at epoch 29 (val_loss=5.9857)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E30] train_loss=5.1378 val_loss=5.9874 BLEU=2.40 CER=0.6842 PPL=398.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E31] train_loss=5.0997 val_loss=5.9663 BLEU=2.58 CER=0.6792 PPL=390.08\n",
            "==> Saved best model at epoch 31 (val_loss=5.9663)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E32] train_loss=5.0623 val_loss=5.9487 BLEU=2.70 CER=0.6482 PPL=383.27\n",
            "==> Saved best model at epoch 32 (val_loss=5.9487)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history, model = run_training(INPUT_DIM, OUTPUT_DIM, emb_dim=128, hid_dim=256,\n",
        "                 enc_layers=1, dec_layers=2,\n",
        "                 dropout=0.1, lr=1e-4, epochs=12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP4DZSz0nVZT",
        "outputId": "b5f7474a-1712-402b-81b8-daeb2e075dbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2667450489.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()   # correct usage\n",
            "train:   0%|          | 0/165 [00:00<?, ?it/s]/tmp/ipython-input-2833786018.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E1] train_loss=7.6501 val_loss=6.7549 BLEU=0.00 CER=0.9709 PPL=858.27\n",
            "==> Saved best model at epoch 1 (val_loss=6.7549)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E2] train_loss=6.3934 val_loss=6.7156 BLEU=0.03 CER=0.7524 PPL=825.21\n",
            "==> Saved best model at epoch 2 (val_loss=6.7156)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E3] train_loss=6.3327 val_loss=6.6718 BLEU=0.04 CER=0.7460 PPL=789.80\n",
            "==> Saved best model at epoch 3 (val_loss=6.6718)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E4] train_loss=6.2777 val_loss=6.6295 BLEU=0.05 CER=0.7889 PPL=757.11\n",
            "==> Saved best model at epoch 4 (val_loss=6.6295)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5] train_loss=6.2256 val_loss=6.6044 BLEU=0.06 CER=0.8146 PPL=738.35\n",
            "==> Saved best model at epoch 5 (val_loss=6.6044)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E6] train_loss=6.1764 val_loss=6.5395 BLEU=0.06 CER=0.8012 PPL=691.94\n",
            "==> Saved best model at epoch 6 (val_loss=6.5395)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E7] train_loss=6.1271 val_loss=6.5003 BLEU=0.07 CER=0.7897 PPL=665.37\n",
            "==> Saved best model at epoch 7 (val_loss=6.5003)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E8] train_loss=6.0756 val_loss=6.4501 BLEU=0.08 CER=0.7636 PPL=632.77\n",
            "==> Saved best model at epoch 8 (val_loss=6.4501)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E9] train_loss=6.0271 val_loss=6.4162 BLEU=0.13 CER=0.7403 PPL=611.66\n",
            "==> Saved best model at epoch 9 (val_loss=6.4162)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E10] train_loss=5.9787 val_loss=6.3810 BLEU=0.14 CER=0.7359 PPL=590.54\n",
            "==> Saved best model at epoch 10 (val_loss=6.3810)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E11] train_loss=5.9357 val_loss=6.3547 BLEU=0.16 CER=0.7480 PPL=575.17\n",
            "==> Saved best model at epoch 11 (val_loss=6.3547)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E12] train_loss=5.8979 val_loss=6.3237 BLEU=0.17 CER=0.7264 PPL=557.64\n",
            "==> Saved best model at epoch 12 (val_loss=6.3237)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history, model = run_training(INPUT_DIM, OUTPUT_DIM, emb_dim=64, hid_dim=128,\n",
        "                 enc_layers=1, dec_layers=2,\n",
        "                 dropout=0.1, lr=1e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UF8teRhppby",
        "outputId": "509ba7cf-ed28-41e9-9823-6288044b58d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2667450489.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()   # correct usage\n",
            "train:   0%|          | 0/165 [00:00<?, ?it/s]/tmp/ipython-input-2833786018.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E1] train_loss=8.4675 val_loss=7.0117 BLEU=0.00 CER=1.0000 PPL=1109.52\n",
            "==> Saved best model at epoch 1 (val_loss=7.0117)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E2] train_loss=6.5243 val_loss=6.7773 BLEU=0.00 CER=1.0000 PPL=877.73\n",
            "==> Saved best model at epoch 2 (val_loss=6.7773)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E3] train_loss=6.4388 val_loss=6.7513 BLEU=0.00 CER=1.0000 PPL=855.13\n",
            "==> Saved best model at epoch 3 (val_loss=6.7513)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E4] train_loss=6.4127 val_loss=6.7309 BLEU=0.00 CER=0.9773 PPL=837.93\n",
            "==> Saved best model at epoch 4 (val_loss=6.7309)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5] train_loss=6.3864 val_loss=6.7085 BLEU=0.02 CER=0.8398 PPL=819.35\n",
            "==> Saved best model at epoch 5 (val_loss=6.7085)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E6] train_loss=6.3623 val_loss=6.6878 BLEU=0.03 CER=0.7690 PPL=802.54\n",
            "==> Saved best model at epoch 6 (val_loss=6.6878)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E7] train_loss=6.3387 val_loss=6.6746 BLEU=0.03 CER=0.7494 PPL=792.05\n",
            "==> Saved best model at epoch 7 (val_loss=6.6746)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E8] train_loss=6.3146 val_loss=6.6515 BLEU=0.04 CER=0.7514 PPL=773.98\n",
            "==> Saved best model at epoch 8 (val_loss=6.6515)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E9] train_loss=6.2976 val_loss=6.6333 BLEU=0.04 CER=0.7634 PPL=759.97\n",
            "==> Saved best model at epoch 9 (val_loss=6.6333)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E10] train_loss=6.2793 val_loss=6.6191 BLEU=0.03 CER=0.7728 PPL=749.27\n",
            "==> Saved best model at epoch 10 (val_loss=6.6191)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E11] train_loss=6.2623 val_loss=6.6055 BLEU=0.04 CER=0.7752 PPL=739.17\n",
            "==> Saved best model at epoch 11 (val_loss=6.6055)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E12] train_loss=6.2466 val_loss=6.5957 BLEU=0.04 CER=0.7777 PPL=731.97\n",
            "==> Saved best model at epoch 12 (val_loss=6.5957)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history, model = run_training(INPUT_DIM, OUTPUT_DIM, emb_dim=64, hid_dim=128,\n",
        "                 enc_layers=1, dec_layers=2,\n",
        "                 dropout=0.5, lr=5e-4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCipgwSvqyC9",
        "outputId": "9cbae5ec-4381-4c8c-9e6d-6ec777bea4b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2667450489.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()   # correct usage\n",
            "train:   0%|          | 0/165 [00:00<?, ?it/s]/tmp/ipython-input-2833786018.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E1] train_loss=7.0309 val_loss=6.7329 BLEU=0.02 CER=0.8380 PPL=839.60\n",
            "==> Saved best model at epoch 1 (val_loss=6.7329)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E2] train_loss=6.3676 val_loss=6.7024 BLEU=0.03 CER=0.7464 PPL=814.34\n",
            "==> Saved best model at epoch 2 (val_loss=6.7024)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E3] train_loss=6.3126 val_loss=6.6971 BLEU=0.04 CER=0.7511 PPL=810.08\n",
            "==> Saved best model at epoch 3 (val_loss=6.6971)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E4] train_loss=6.2728 val_loss=6.6990 BLEU=0.05 CER=0.7569 PPL=811.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E5] train_loss=6.2292 val_loss=6.6316 BLEU=0.05 CER=0.7626 PPL=758.73\n",
            "==> Saved best model at epoch 5 (val_loss=6.6316)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E6] train_loss=6.1850 val_loss=6.5592 BLEU=0.06 CER=0.7799 PPL=705.70\n",
            "==> Saved best model at epoch 6 (val_loss=6.5592)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E7] train_loss=6.1361 val_loss=6.5183 BLEU=0.07 CER=0.8055 PPL=677.41\n",
            "==> Saved best model at epoch 7 (val_loss=6.5183)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E8] train_loss=6.0846 val_loss=6.4764 BLEU=0.08 CER=0.7702 PPL=649.63\n",
            "==> Saved best model at epoch 8 (val_loss=6.4764)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E9] train_loss=6.0288 val_loss=6.4285 BLEU=0.10 CER=0.7784 PPL=619.24\n",
            "==> Saved best model at epoch 9 (val_loss=6.4285)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E10] train_loss=5.9688 val_loss=6.3811 BLEU=0.11 CER=0.7261 PPL=590.57\n",
            "==> Saved best model at epoch 10 (val_loss=6.3811)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E11] train_loss=5.9037 val_loss=6.3345 BLEU=0.23 CER=0.7353 PPL=563.66\n",
            "==> Saved best model at epoch 11 (val_loss=6.3345)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E12] train_loss=5.8345 val_loss=6.2885 BLEU=0.20 CER=0.7219 PPL=538.37\n",
            "==> Saved best model at epoch 12 (val_loss=6.2885)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history, model = run_training(INPUT_DIM, OUTPUT_DIM, emb_dim=64, hid_dim=128,\n",
        "                 enc_layers=1, dec_layers=2,\n",
        "                 dropout=0.5, lr=1e-4, resume_from=\"/content/best_checkpoint.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPn-7CphrjDm",
        "outputId": "7ed3c50a-b281-4a02-83ff-1adfe7dba6d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2667450489.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()   # correct usage\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumed training from epoch 12 using /content/best_checkpoint.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train:   0%|          | 0/165 [00:00<?, ?it/s]/tmp/ipython-input-2833786018.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E13] train_loss=5.7301 val_loss=6.2475 BLEU=0.24 CER=0.7187 PPL=516.74\n",
            "==> Saved best model at epoch 13 (val_loss=6.2475)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E14] train_loss=5.6489 val_loss=6.2022 BLEU=0.39 CER=0.6974 PPL=493.82\n",
            "==> Saved best model at epoch 14 (val_loss=6.2022)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E15] train_loss=5.5722 val_loss=6.1517 BLEU=0.45 CER=0.6695 PPL=469.52\n",
            "==> Saved best model at epoch 15 (val_loss=6.1517)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E16] train_loss=5.4996 val_loss=6.1099 BLEU=0.77 CER=0.6825 PPL=450.29\n",
            "==> Saved best model at epoch 16 (val_loss=6.1099)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E17] train_loss=5.4304 val_loss=6.0709 BLEU=0.81 CER=0.6900 PPL=433.09\n",
            "==> Saved best model at epoch 17 (val_loss=6.0709)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E18] train_loss=5.3583 val_loss=6.0338 BLEU=1.23 CER=0.6652 PPL=417.31\n",
            "==> Saved best model at epoch 18 (val_loss=6.0338)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E19] train_loss=5.2972 val_loss=6.0337 BLEU=1.64 CER=0.6890 PPL=417.27\n",
            "==> Saved best model at epoch 19 (val_loss=6.0337)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E20] train_loss=5.2294 val_loss=5.9770 BLEU=1.59 CER=0.6607 PPL=394.27\n",
            "==> Saved best model at epoch 20 (val_loss=5.9770)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E21] train_loss=5.1681 val_loss=5.9533 BLEU=2.15 CER=0.6640 PPL=385.01\n",
            "==> Saved best model at epoch 21 (val_loss=5.9533)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E22] train_loss=5.1092 val_loss=5.9566 BLEU=2.24 CER=0.6720 PPL=386.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E23] train_loss=5.0481 val_loss=5.9089 BLEU=2.64 CER=0.6434 PPL=368.30\n",
            "==> Saved best model at epoch 23 (val_loss=5.9089)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[E24] train_loss=4.9896 val_loss=5.9063 BLEU=3.08 CER=0.6563 PPL=367.33\n",
            "==> Saved best model at epoch 24 (val_loss=5.9063)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history, model = run_training(INPUT_DIM, OUTPUT_DIM, emb_dim=64, hid_dim=64,\n",
        "                 enc_layers=1, dec_layers=2,\n",
        "                 dropout=0.5, lr=5e-4,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "7OlnSNA0srZ-",
        "outputId": "9b58bdd2-acbe-4d12-9431-7b00a8b4db19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2667450489.py:18: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()   # correct usage\n",
            "train:   0%|          | 0/165 [00:00<?, ?it/s]/tmp/ipython-input-2833786018.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "DecoderLSTM.forward_step() takes 4 positional arguments but 5 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1010945647.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history, model = run_training(INPUT_DIM, OUTPUT_DIM, emb_dim=64, hid_dim=64,\n\u001b[0m\u001b[1;32m      2\u001b[0m                  \u001b[0menc_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  dropout=0.5, lr=5e-4,)\n",
            "\u001b[0;32m/tmp/ipython-input-2667450489.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(INPUT_DIM, OUTPUT_DIM, emb_dim, hid_dim, enc_layers, dec_layers, dropout, lr, epochs, save_dir, resume_from)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# teacher forcing decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_id2tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mbleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2833786018.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, criterion, scaler, tf_ratio)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mout_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4031982179.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_len, trg, teacher_forcing, max_len)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: DecoderLSTM.forward_step() takes 4 positional arguments but 5 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history, model = run_training(\n",
        "    INPUT_DIM, OUTPUT_DIM,\n",
        "    emb_dim=64, hid_dim=128,\n",
        "    enc_layers=1, dec_layers=2,\n",
        "    dropout=0.5, lr=1e-4\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "kKSqLXBothGm",
        "outputId": "893ac19b-43be-48e9-d78a-81b8d712eb78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2153293004.py:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()   # AMP scaler\n",
            "train:   0%|          | 0/165 [00:00<?, ?it/s]/tmp/ipython-input-2833786018.py:40: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected hidden[0] size (2, 64, 128), got [1, 64, 128]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-946769423.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history, model = run_training(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mINPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhid_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0menc_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2153293004.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(INPUT_DIM, OUTPUT_DIM, emb_dim, hid_dim, enc_layers, dec_layers, dropout, lr, epochs, save_dir, resume_from)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# teacher forcing decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_id2tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mbleu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2833786018.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, criterion, scaler, tf_ratio)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mout_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2904171676.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_len, trg, teacher_forcing, max_len)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2904171676.py\u001b[0m in \u001b[0;36mforward_step\u001b[0;34m(self, input_token, hidden, cell, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [b,1,hid]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mrnn_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# [b,1,emb+hid]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0;31m# Each batch of the hidden state should match the input sequence that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m                 \u001b[0;31m# the user believes he/she is passing in.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m                 \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m   1001\u001b[0m     ):\n\u001b[1;32m   1002\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m         self.check_hidden_size(\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    346\u001b[0m     ) -> None:\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_weights_have_changed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (2, 64, 128), got [1, 64, 128]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IlO1kH9IueLu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}